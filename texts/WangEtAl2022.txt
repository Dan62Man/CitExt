Title: UNSUPERVISED KNOWLEDGE GRAPH CONSTRUCTION AND EVENT-CENTRIC KNOWLEDGE INFUSION FOR SCIENTIFIC NLI
Natural language inference (NLI), an essential task for natural language understanding, aims to deduce relationship between the given premise and hypothesis [1]. NLI is a fundamental problem in many natural language processing (NLP)
tasks, such as sentence embeddings [2], question answering
[3] and commonsense reasoning [4]. Therefore, it has been
widely concerned by many researchers. With the widespread
NLP application, a rising demand for NLI methods is to handle specific-domain text, such as scientific texts [5], medical articles [6] and financial news [7]. To build a large NLI
dataset related to scientific texts, SciNLI dataset [5] is built
from scholarly papers on NLP and computational linguistics.
Due to the success of pre-trained language models (PTM)
(e.g., BERT [8] and RoBERTa [9]), a general paradigm for
NLI is to fine-tune a PTM on downstream dataset. However, PTMs fine-tuned on specific-domain data often suffer
from a cross-domain problem since they are pre-trained on
general domain corpora such as news articles and Wikipedia.
Many works generalize PTM to specific-domain via pre-train
on specific-domain corpus [10, 11] or introducing specificdomain knowledge graph [12]. Although Beltagy et al. [10]
consume enormous training resources to exclusively pre-train
SciBERT on scientific texts, RoBERTa with a more sophisticated pre-training leads to better performance. Therefore,
it is necessary to introduce a scientific knowledge graph to
generalize PTM to scientific domain.
Recently, some works [13, 14] propose building scientific
knowledge graphs automatically via training entity recognition and relationship extraction models with labeled data. Despite their success, they still suffer from some drawbacks,
i.e., expensive labeled data, failure to apply in other domains,
long inference time 1
and difficulty extending to large corpora.
Specifically, existing methods [13, 14] are trained on only 500
abstracts and show an undesirable performance (i.e., 44.7 on
F1). Therefore, we efficiently and easily build a scientific
knowledge graph (SKG) without any labeled data. We first
parse scientific texts via a dependency parser and then extract
the subjects, predicates and objects as triplet candidates. To
reduce noise samples in triplet candidates, we propose heuristic filtering methods to improve the accuracy of triplets. Due
to fast inference and not requiring any labeled data and training sources, our method can easy to extend to large corpora
in other domains.
To improve PTM reasoning, previous works [15] mainly
use sentences as queries to retrieve triplets in knowledge
graph and integrate them into PTM. However, since there
are still some noise samples in SKG, directly integrating
knowledge into model damages model learning. Recently,
event-centric reasoning [16, 17] shows powerful reasoning
capability in context via understanding correlation between
events. To reduce the effect of noise data and complement
knowledge in sentences better, we propose an event-centric
knowledge infusion framework. Precisely, we follow [16] to
split events into sentences and then use all events as queries
to retrieve relevant triplets, which can prevent the retrieved
knowledge only relevant to limited semantics in sentences.
Moreover, we integrate knowledge into multiple event units,
improving context reasoning via enriching semantic information in each event.
We conduct an extensive evaluation on SciNLI dataset [5].
Results show that our method achieves state-of-the-art performance compared with other methods. In addition, we analyze
the effectiveness and reliability of SKG.