[1] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning, “A large annotated corpus for learning natural language inference,” in EMNLP 2015. 2015, pp. 632–642, The Association for Computational Linguistics.
[2] Tianyu Gao, Xingcheng Yao, and Danqi Chen, “Simcse: Simple contrastive learning of sentence embeddings,” in EMNLP 2021. 2021, pp. 6894–6910, Association for Computational Linguistics.
[3] Ming Tan, C´ıcero Nogueira dos Santos, Bing Xiang, and Bowen Zhou, “Improved representation learning for question answer matching,” in ACL 2016. 2016, The Association for Computer Linguistics.
[4] Maarten Sap, Vered Shwartz, Antoine Bosselut, Yejin Choi, and Dan Roth, “Commonsense reasoning for natural language processing,” in ACL 2020. 2020, pp. 27–33, Association for Computational Linguistics.
[5] Mobashir Sadat and Cornelia Caragea, “Scinli: A corpus for natural language inference on scientific text,” in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022. 2022, pp. 7399–7409, Association for Computational Linguistics.
[6] Yun He, Ziwei Zhu, Yin Zhang, Qin Chen, and James Caverlee, “Infusing disease knowledge into BERT for health question answering, medical inference and disease name recognition,” in EMNLP 2020. 2020, pp. 4604–4614, Association for Computational Linguistics.
[7] Robert P. Schumaker and Hsinchun Chen, “Textual analysis of stock market prediction using breaking financial news: The azfin text system,” ACM Trans. Inf. Syst., vol. 27, no. 2, pp. 12:1–12:19, 2009.
[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, “BERT: pre-training of deep bidirectional transformers for language understanding,” in NAACL-HLT 2019. 2019, pp. 4171–4186, Association for Computational Linguistics.
[9] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov, “Roberta: A robustly optimized BERT pretraining approach,” CoRR, vol. abs/1907.11692, 2019.
[10] Iz Beltagy, Kyle Lo, and Arman Cohan, “Scibert: A pretrained language model for scientific text,” in EMNLPIJCNLP 2019. 2019, pp. 3613–3618, Association for Computational Linguistics.
[11] Yucheng Zhou, “Sketch storytelling,” in IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore, 23- 27 May 2022. 2022, pp. 4748–4752, IEEE.
[12] Yucheng Zhou, Xiubo Geng, Tao Shen, Jian Pei, Wenqiang Zhang, and Daxin Jiang, “Modeling event-pair relations in external knowledge graphs for script reasoning,” in Findings of the Association for Computational inguistics: ACL-IJCNLP 2021, Online, Aug. 2021, pp. 4586–4596, Association for Computational Linguistics.
[13] Isabelle Augenstein, Mrinal Das, Sebastian Riedel, Lakshmi Vikraman, and Andrew McCallum, “Semeval 2017 task 10: Scienceie - extracting keyphrases and relations from scientific publications,” in SemEval@ACL 2017. 2017, pp. 546–555, Association for Computational Linguistics.
[14] Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi, “Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction,” in EMNLP 2018. 2018, pp. 3219–3232, Association for Computational Linguistics.
[15] Zikang Wang, Linjing Li, and Daniel Zeng, “Knowledge-enhanced natural language inference based on knowledge graphs,” in COLING 2020. 2020, pp. 6498–6508, International Committee on Computational Linguistics.
[16] Yucheng Zhou, Xiubo Geng, Tao Shen, Guodong Long, and Daxin Jiang, “Eventbert: A pre-trained model for event correlation reasoning,” in WWW ’22: The ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022. 2022, pp. 850–859, ACM.
[17] Yucheng Zhou, Tao Shen, Xiubo Geng, Guodong Long, and Daxin Jiang, “Claret: Pre-training a correlationaware context-to-event transformer for event-centric generation and classification,” in ACL 2022. 2022, pp. 2559–2575, Association for Computational Linguistics.
[18] Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David McClosky, “The stanford corenlp natural language processing toolkit,” in ACL 2014. 2014, pp. 55–60, The Association for Computer Linguistics.
[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin, “Attention is all you need,” in Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, 2017, pp. 5998–6008.