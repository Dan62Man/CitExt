{
    "id": "BosselutEtAl2019",
    "sentences": [
        {
            "-1": [
                "In this work, we cast commonsense acquisition as knowledge base construction and investigate whether large-scale language models can effectively learn to generate the knowledge necessary to automatically construct a commonsense knowledge base (KB).",
                0
            ],
            "0": [
                "Automatic KB construction is a long-standing goal of artificial intelligence research due to the difficulty of achieving high concept coverage in high-precision curated KBs (Lenat, 1995; Miller, 1995).",
                0
            ],
            "final_score": 0
        },
        {
            "0": [
                "Previous work has developed models capable of reading and extracting semi-structured text (Suchanek et al., 2007; Hoffart et al., 2013; Auer et al., 2007; Bollacker et al., 2008) and unstructured text (Dong et al., 2014; Carlson et al., 2010; Nakashole et al., 2011, 2012; Niu, 2012) into relational schemas that can be queried for downstream applications.",
                0
            ],
            "1": [
                "A common thread of these approaches, however, is the focus on encyclopedic knowledge, which lends itself to a well-defined space of entities and relations that can be modeled.",
                0
            ],
            "final_score": 0
        },
        {
            "-1": [
                "A common thread of these approaches, however, is the focus on encyclopedic knowledge, which lends itself to a well-defined space of entities and relations that can be modeled.",
                0
            ],
            "0": [
                "Commonsense knowledge, however, does not cleanly fit into a schema comparing two entities with a known relation, leading current approaches to model \"entities\" as natural language phrases and relations as any concept that can link them (Li et al., 2016; Sap et al., 2019).",
                0
            ],
            "final_score": 0
        },
        {
            "0": [
                "OpenIE approaches display this property of open text entities and relations (Etzioni et al., 2011; Fader et al., 2011; Mausam et al., 2012), but being extractive, they only capture knowledge that is explicitly mentioned in text, limiting their applicability for capturing commonsense knowledge, which is often implicit (Gordon and Van Durme, 2013).",
                0
            ],
            "final_score": 0
        },
        {
            "0": [
                "Meanwhile, recent progress in training deep contextualized language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018) provides an opportunity to explore beyond extractive methods as an avenue for commonsense KB construction.",
                0
            ],
            "1": [
                "These large-scale language models display impressive performance when their underlying representations are tuned to solve end tasks, achieving state-of-the-art results on a variety of complex problems.",
                1
            ],
            "final_score": 1
        }
    ]
}