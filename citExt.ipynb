{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a82d3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install json\n",
    "!pip install spacy\n",
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_scibert-0.5.1.tar.gz\n",
    "!pip install transformers\n",
    "!pip install pyvis\n",
    "!pip install spacy-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "848aea9b-277e-4ec7-bba4-16ea3e15a227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macac\\AppData\\Local\\Temp\\ipykernel_15400\\2481806785.py:11: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bee3b46c044cf38abde1f77dad676c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\macac\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\macac\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779c4ab480c64532a7a6de8408f3f2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at j-hartmann/sentiment-roberta-large-english-3-classes were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23042075fafc4795ae560b15b5f80718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e91483e4a3a44998c937ea72b46bef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3662195caea466d8c70ead34cfbcf32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d746039918c24d62831af02434589d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d68b358942543afbec5e5c6a2c715f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/688 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e630b1bb50f4624838f8d989b8d1ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fdc815fddc44f097c23157e9f90fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2ae68bdf624742b6e3f2ea9ca58d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c398f3a62c4061b92bf7b04e8ed0a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________________________________________________________________\n",
      "Reading ./texts\\AburaedEtAl2017.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\macac\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Citation sections written to ./jsonFiles\\AburaedEtAl2017.json\n",
      "_________________________________________________________________________________________________________________________\n",
      "Reading ./texts\\AburaedEtAl2018.txt\n",
      "  Citation sections written to ./jsonFiles\\AburaedEtAl2018.json\n",
      "_________________________________________________________________________________________________________________________\n",
      "Reading ./texts\\AburaedEtAl2020.txt\n",
      "  Citation sections written to ./jsonFiles\\AburaedEtAl2020.json\n",
      "_________________________________________________________________________________________________________________________\n",
      "Reading ./texts\\BosselutEtAl2019.txt\n",
      "  Citation sections written to ./jsonFiles\\BosselutEtAl2019.json\n",
      "_________________________________________________________________________________________________________________________\n",
      "Reading ./texts\\CarlsonEtAl2010.txt\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 637: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 278\u001b[0m\n\u001b[0;32m    276\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mbibliography\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m doc:\n\u001b[0;32m    277\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m_________________________________________________________________________________________________________________________\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 278\u001b[0m     save_citation_sections(doc, output_folder, nlp)\n\u001b[0;32m    280\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCitations extracted!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    282\u001b[0m json_docs \u001b[39m=\u001b[39m get_docs(output_folder)\n",
      "Cell \u001b[1;32mIn[2], line 224\u001b[0m, in \u001b[0;36msave_citation_sections\u001b[1;34m(text_file, json_dir, nlp)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_citation_sections\u001b[39m(text_file, json_dir, nlp):\n\u001b[1;32m--> 224\u001b[0m     text, paper_id \u001b[39m=\u001b[39m extract_citing_data(text_file)\n\u001b[0;32m    226\u001b[0m     citation_json \u001b[39m=\u001b[39m text_to_json(text, nlp)\n\u001b[0;32m    228\u001b[0m     json_data \u001b[39m=\u001b[39m {\n\u001b[0;32m    229\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m: paper_id,\n\u001b[0;32m    230\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msentences\u001b[39m\u001b[39m\"\u001b[39m: citation_json\n\u001b[0;32m    231\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[2], line 252\u001b[0m, in \u001b[0;36mextract_citing_data\u001b[1;34m(text_file)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_citing_data\u001b[39m(text_file):\n\u001b[1;32m--> 252\u001b[0m   text, title \u001b[39m=\u001b[39m extract_text_from_file(text_file)\n\u001b[0;32m    253\u001b[0m   paper_id \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(text_file)\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    254\u001b[0m   author, publication_year \u001b[39m=\u001b[39m get_citing_author_year (paper_id)\n",
      "Cell \u001b[1;32mIn[2], line 214\u001b[0m, in \u001b[0;36mextract_text_from_file\u001b[1;34m(text_file)\u001b[0m\n\u001b[0;32m    211\u001b[0m title \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    213\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(text_file, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m--> 214\u001b[0m     text \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39;49mread()\n\u001b[0;32m    216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mTitle: \u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m text:\n\u001b[0;32m    217\u001b[0m   title \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mTitle: \u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\macac\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39mcharmap_decode(\u001b[39minput\u001b[39m,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors,decoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 637: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "from sys import stdin\n",
    "import os\n",
    "import json\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import spacy\n",
    "import pyvis\n",
    "from pyvis import network as net\n",
    "import networkx as nx\n",
    "from IPython.core.display import display, HTML\n",
    "sys.path.append(\"code\")\n",
    "import citation_data_manipulation as manipulate_data\n",
    "\n",
    "# Prepare a pipeline for sentiment-analysis\n",
    "classifier = pipeline(\"text-classification\", model=\"j-hartmann/sentiment-roberta-large-english-3-classes\")\n",
    "\n",
    "# Prepare for Natural Language Inference (NLI) using nli-roberta-base\n",
    "inference = pipeline('zero-shot-classification', model='roberta-large-mnli')\n",
    "inference_labels = ['entailment', 'neutral', 'contradiction']\n",
    "\n",
    "# Load en_core_sci_scibert model to separate sentences\n",
    "nlp = spacy.load('en_core_sci_scibert')\n",
    "\n",
    "# Prepare Name Entity Recognition (NER)\n",
    "ner = pipeline(\"token-classification\", model=\"Jean-Baptiste/roberta-large-ner-english\", aggregation_strategy=\"simple\")\n",
    "\n",
    "# input\n",
    "input_folder = './texts'\n",
    "output_folder = \"./jsonFiles\"\n",
    "\n",
    "#Regex necessary for identifying citations in text\n",
    "author = \"(?:[A-Z][A-Za-z'`-]+)\"\n",
    "etal = \"(?:et al.?)\"\n",
    "additional = \"(?:,? (?:(?:and |& )?\" + author + \"|\" + etal + \"))\"\n",
    "year_num = \"(?:19|20)[0-9][0-9]\"\n",
    "page_num = \"(?:, p.? [0-9]+)?\"\n",
    "year = \"(?:,? *\" + year_num + page_num + \"| *\\(\" + year_num + page_num + \"\\))\"\n",
    "name_year_regex = \"(\" + author + additional + \"*\" + year + \")\"\n",
    "\n",
    "#In case the regex type above is not used we assume the regex might look like this: [1]\n",
    "num_bracket_regex = r\"(\\[\\d+\\])\"\n",
    "regex = name_year_regex + \"|\" + num_bracket_regex\n",
    "graphs = []\n",
    "# For paper identification\n",
    "paper_data = {}\n",
    "\n",
    "# Start of json to node and edge creation\n",
    "\n",
    "def get_cited_papers(paper_id, sentence):\n",
    "  using_et_al = False\n",
    "  if '[' in sentence:\n",
    "    citations = re.findall(num_bracket_regex, sentence)\n",
    "  else:\n",
    "    citations = re.findall(name_year_regex, sentence)\n",
    "    using_et_al = True\n",
    "  global_ids = []\n",
    "\n",
    "  for citation in citations:\n",
    "    if using_et_al:\n",
    "      cit_id = citation\n",
    "    else:\n",
    "      cit_id = citation[1].replace('[','').replace(']','')\n",
    "    global_ids.append(manipulate_data.append_reference_data(paper_data, paper_id, cit_id))\n",
    "\n",
    "  return global_ids\n",
    "\n",
    "\n",
    "def id_sentiment_id(citer, sentences):\n",
    "  entities_relations = []\n",
    "\n",
    "  for sentence in sentences:\n",
    "    score = sentence[\"final_score\"]\n",
    "    cited_papers = sentence[\"0\"][2]\n",
    "    for cited in cited_papers:\n",
    "      citation_relations = [citer, score, cited]\n",
    "      entities_relations.append(citation_relations)\n",
    "\n",
    "  return entities_relations\n",
    "# End of json to node and edge creation\n",
    "\n",
    "\n",
    "# Start of KG generation\n",
    "def gradient_color(mapped_value):\n",
    "    # Define the color points and colors for the gradient\n",
    "    color_points = [-1, -0.5, 0.5]\n",
    "    colors = ['#9B3131', '#3F7190', '#1D9A60']\n",
    "    color = ''\n",
    "\n",
    "    for i in range(3):\n",
    "      if mapped_value > color_points[i]:\n",
    "        color = colors[i]\n",
    "\n",
    "    return color\n",
    "\n",
    "\n",
    "def get_cited_from_global_id(global_id):\n",
    "  if isinstance(global_id, int):\n",
    "    author = paper_data[global_id][\"Author\"]\n",
    "    title = paper_data[global_id][\"Title\"]\n",
    "    year = paper_data[global_id][\"Year\"]\n",
    "    title_first_words = ' '.join(title.split()[:3])\n",
    "    node = f\"{author}, {year}, {title_first_words}...\"\n",
    "    return node\n",
    "  return global_id\n",
    "\n",
    "\n",
    "def get_ref_code(citation_id):\n",
    "  for paper in paper_data:\n",
    "    data = paper_data[paper]\n",
    "    if data[\"ID\"] == citation_id:\n",
    "      return paper\n",
    "\n",
    "  return len(paper_data)\n",
    "\n",
    "\n",
    "def visualize_KG(html_name):\n",
    "  g=net.Network(notebook=True, cdn_resources='in_line')#, directed=True)\n",
    "\n",
    "  for graph in graphs:\n",
    "    for nodes_rel in graph:\n",
    "      citer = nodes_rel[0]\n",
    "      cited = get_cited_from_global_id(nodes_rel[2])\n",
    "\n",
    "      rel_color = gradient_color(nodes_rel[1])\n",
    "      edge_width = abs(nodes_rel[1]) + 0.5\n",
    "\n",
    "      citer_ref = get_ref_code(citer)\n",
    "      cited_ref = nodes_rel[2]\n",
    "\n",
    "      citer = get_cited_from_global_id(citer)\n",
    "\n",
    "      g.add_node(citer_ref, label = citer)\n",
    "      g.add_node(cited_ref, label = cited, color = rel_color)\n",
    "      g.add_edge(citer_ref, cited_ref, color = rel_color, width = (edge_width*edge_width))\n",
    "\n",
    "  g.show(html_name)\n",
    "  display(HTML(html_name))\n",
    "# End of KG generation\n",
    "\n",
    "\n",
    "def prepare_entities(documents):\n",
    "  for doc in documents:\n",
    "    # Read the JSON data from the file\n",
    "    with open(doc, 'r') as json_file:\n",
    "      data = json.load(json_file)\n",
    "\n",
    "    id = data[\"id\"]\n",
    "    sentences = data[\"sentences\"]\n",
    "\n",
    "    graph = id_sentiment_id(id, sentences)\n",
    "    graphs.append(graph)\n",
    "\n",
    "\n",
    "def get_docs(documents_folder):\n",
    "    files = []\n",
    "    for file_name in os.listdir(documents_folder):\n",
    "        file_path = os.path.join(documents_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            files.append(file_path)\n",
    "    return files\n",
    "\n",
    "\n",
    "def has_citation(text):\n",
    "    pattern = re.compile(regex)\n",
    "    match = pattern.search(text)\n",
    "    return match is not None\n",
    "\n",
    "\n",
    "def format_sentiment_result(sen):\n",
    "    data = classifier(sen.text)[0]\n",
    "    sentiment = data[\"label\"]\n",
    "    value = data[\"score\"]\n",
    "\n",
    "    if sentiment == 'negative':\n",
    "      value = -value\n",
    "    elif sentiment == 'neutral':\n",
    "      value = 0\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "def get_context(sen, inferring_sen):\n",
    "  sequence_to_classify = sen + \" \" + inferring_sen\n",
    "\n",
    "  result = inference(sequence_to_classify, inference_labels)\n",
    "  labels = result['labels']\n",
    "\n",
    "  return labels[0]\n",
    "\n",
    "\n",
    "def get_contextual_result(sentence, context, is_prev_sentence):\n",
    "  result_modifier = 1\n",
    "  sentiment_value = format_sentiment_result(sentence)\n",
    "  if context == 'contradiction':\n",
    "    if is_prev_sentence:\n",
    "      result_modifier = -1\n",
    "  else:\n",
    "    if not is_prev_sentence:\n",
    "      result_modifier = -1\n",
    "\n",
    "  return result_modifier * sentiment_value\n",
    "\n",
    "\n",
    "def prepare_citation_sections(paper_id, sens):\n",
    "  prev_sen = None\n",
    "  prev_context_value = next_context_value = 0\n",
    "  citation_json = []\n",
    "\n",
    "  for sen, next_sen in zip(sens, sens[1:] + [None]):\n",
    "    if has_citation(sen.text):\n",
    "      sen_value = format_sentiment_result(sen)\n",
    "      cited_papers = get_cited_papers(paper_id, sen.text)\n",
    "      citation_section = {}\n",
    "\n",
    "      if prev_sen and not has_citation(prev_sen.text):\n",
    "        context = get_context(prev_sen.text, sen.text)\n",
    "        if not context == 'neutral':\n",
    "          prev_context_value = get_contextual_result(prev_sen, context, True)\n",
    "          citation_section[-1] = [prev_sen.text, prev_context_value]\n",
    "\n",
    "      citation_section[0] = [sen.text, sen_value, cited_papers]\n",
    "\n",
    "      if next_sen and not has_citation(next_sen.text):\n",
    "        context = get_context(sen.text, next_sen.text)\n",
    "        if not context == 'neutral':\n",
    "          next_context_value = get_contextual_result(next_sen, context, False)\n",
    "          citation_section[1] = [next_sen.text, next_context_value]\n",
    "\n",
    "      citation_section[\"final_score\"] = sen_value + prev_context_value + next_context_value\n",
    "      citation_json.append(citation_section)\n",
    "\n",
    "  prev_sen = sen\n",
    "\n",
    "  return citation_json\n",
    "\n",
    "\n",
    "def text_to_json(paper_id, text, nlp):\n",
    "    doc = nlp(text)\n",
    "    sens = list(doc.sents)\n",
    "\n",
    "    citation_json = prepare_citation_sections(paper_id, sens)\n",
    "\n",
    "    return citation_json\n",
    "\n",
    "\n",
    "def extract_text_from_file(text_file):\n",
    "    print(f\"Reading {text_file}\")\n",
    "    title = ''\n",
    "\n",
    "    with open(text_file, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    if 'Title: ' in text:\n",
    "      title = text.split('\\n')[0].replace('Title: ', '')\n",
    "      text = text.split(\"\\n\",1)[1].replace('\\n', ' ')\n",
    "\n",
    "    return text, title\n",
    "\n",
    "\n",
    "def save_citation_sections(text_file, json_dir, nlp):\n",
    "    text, paper_id = extract_citing_data(text_file)\n",
    "\n",
    "    citation_json = text_to_json(paper_id, text, nlp)\n",
    "\n",
    "    json_data = {\n",
    "        \"id\": paper_id,\n",
    "        \"sentences\": citation_json\n",
    "    }\n",
    "\n",
    "    file_path = os.path.join(json_dir, paper_id+\".json\")\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(json_data, file, indent=4)\n",
    "\n",
    "    print(f\"  Citation sections written to {file_path}\")\n",
    "\n",
    "\n",
    "def get_citing_author_year(citing_paper_id):\n",
    "  publication_year = re.search(r\"\\d{4}$\", citing_paper_id).group()\n",
    "\n",
    "  author =  citing_paper_id.replace(publication_year, '')\n",
    "\n",
    "  author = re.sub(\"EtAl$\", '', author, flags=re.IGNORECASE).replace(r\".*And.*\", \" and \")\n",
    "\n",
    "  return author, publication_year\n",
    "\n",
    "\n",
    "def extract_citing_data(text_file):\n",
    "  text, title = extract_text_from_file(text_file)\n",
    "  paper_id = os.path.basename(text_file).replace(\".txt\", \"\")\n",
    "  author, publication_year = get_citing_author_year(paper_id)\n",
    "\n",
    "  paper_data[len(paper_data)] = {\n",
    "      \"ID\": paper_id,\n",
    "      \"Author\": author,\n",
    "      \"Title\": title,\n",
    "      \"Year\": publication_year\n",
    "  }\n",
    "\n",
    "  return text, paper_id\n",
    "\n",
    "\n",
    "docs = get_docs(input_folder)\n",
    "cnt = 1\n",
    "if os.path.exists(output_folder):\n",
    "  while os.path.exists(f\"{output_folder}_({cnt})\"):\n",
    "    cnt += 1\n",
    "  output_folder = output_folder + f\"_({cnt})\"\n",
    "\n",
    "os.makedirs(output_folder)\n",
    "\n",
    "for doc in docs:\n",
    "  if not 'bibliography' in doc:\n",
    "    print(\"_________________________________________________________________________________________________________________________\")\n",
    "    save_citation_sections(doc, output_folder, nlp)\n",
    "\n",
    "print(\"Citations extracted!\")\n",
    "\n",
    "json_docs = get_docs(output_folder)\n",
    "prepare_entities(json_docs)\n",
    "\n",
    "print(paper_data)\n",
    "with open('citations.json', \"w\") as file:\n",
    "  json.dump(paper_data, file, indent=2)\n",
    "\n",
    "visualize_KG('WIP_KG.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
