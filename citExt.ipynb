{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a82d3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install json\n",
    "!pip install spacy\n",
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_scibert-0.5.1.tar.gz\n",
    "!pip install transformers\n",
    "!pip install pyvis\n",
    "!pip install spacy-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "848aea9b-277e-4ec7-bba4-16ea3e15a227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macac\\AppData\\Local\\Temp\\ipykernel_28572\\3426310607.py:12: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n",
      "Some weights of the model checkpoint at j-hartmann/sentiment-roberta-large-english-3-classes were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________________________________________________________________\n",
      "Reading .\\texts\\YousifEtAl2017.txt\n",
      "  Citation sections written to .\\jsonFiles\\YousifEtAl2017.json\n",
      "Citations extracted!\n",
      "small_presentation_demo.html\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode characters in position 263607-263621: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\macac\\CitExt\\citExt.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/macac/CitExt/citExt.ipynb#W1sZmlsZQ%3D%3D?line=371'>372</a>\u001b[0m   \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mcitations.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/macac/CitExt/citExt.ipynb#W1sZmlsZQ%3D%3D?line=372'>373</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(paper_data, file, indent\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/macac/CitExt/citExt.ipynb#W1sZmlsZQ%3D%3D?line=374'>375</a>\u001b[0m visualize_KG(\u001b[39m'\u001b[39;49m\u001b[39msmall_presentation_demo.html\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\macac\\CitExt\\citExt.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/macac/CitExt/citExt.ipynb#W1sZmlsZQ%3D%3D?line=169'>170</a>\u001b[0m       edge_label \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mNo Named Entity found!\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/macac/CitExt/citExt.ipynb#W1sZmlsZQ%3D%3D?line=170'>171</a>\u001b[0m     g\u001b[39m.\u001b[39madd_edge(citer_ref, cited_ref, color \u001b[39m=\u001b[39m rel_color, width \u001b[39m=\u001b[39m (edge_width\u001b[39m*\u001b[39medge_width), title \u001b[39m=\u001b[39m edge_label)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/macac/CitExt/citExt.ipynb#W1sZmlsZQ%3D%3D?line=172'>173</a>\u001b[0m g\u001b[39m.\u001b[39;49mshow(html_name, \u001b[39mTrue\u001b[39;49;00m, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/macac/CitExt/citExt.ipynb#W1sZmlsZQ%3D%3D?line=173'>174</a>\u001b[0m display(HTML(html_name))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyvis\\network.py:548\u001b[0m, in \u001b[0;36mNetwork.show\u001b[1;34m(self, name, local, notebook)\u001b[0m\n\u001b[0;32m    546\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite_html(name, open_browser\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,notebook\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    547\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 548\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_html(name, open_browser\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    549\u001b[0m \u001b[39mif\u001b[39;00m notebook:\n\u001b[0;32m    550\u001b[0m     \u001b[39mreturn\u001b[39;00m IFrame(name, width\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth, height\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheight)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyvis\\network.py:530\u001b[0m, in \u001b[0;36mNetwork.write_html\u001b[1;34m(self, name, local, notebook, open_browser)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcdn_resources \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39min_line\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcdn_resources \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mremote\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    529\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(getcwd_name, \u001b[39m\"\u001b[39m\u001b[39mw+\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m out:\n\u001b[1;32m--> 530\u001b[0m         out\u001b[39m.\u001b[39;49mwrite(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhtml)\n\u001b[0;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcdn_resources is not in [\u001b[39m\u001b[39m'\u001b[39m\u001b[39min_line\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39mremote\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlocal\u001b[39m\u001b[39m'\u001b[39m\u001b[39m].\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\encodings\\cp1252.py:19\u001b[0m, in \u001b[0;36mIncrementalEncoder.encode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39;49mcharmap_encode(\u001b[39minput\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,encoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode characters in position 263607-263621: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "from sys import stdin\n",
    "import os\n",
    "import json\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import spacy\n",
    "import pyvis\n",
    "from pyvis import network as net\n",
    "import networkx as nx\n",
    "from IPython.core.display import display, HTML\n",
    "sys.path.append(\"code\")\n",
    "import citation_data_manipulation as manipulate_data\n",
    "\n",
    "# Check if gpu is available\n",
    "gpu_available = torch.cuda.is_available()\n",
    "\n",
    "# Prepare various pipelines based on whether gpu is available\n",
    "if gpu_available:\n",
    "  print(\"Using GPU\")\n",
    "# Prepare a pipeline for sentiment-analysis\n",
    "  classifier = pipeline(\"text-classification\", model=\"j-hartmann/sentiment-roberta-large-english-3-classes\", device=0)\n",
    "# Prepare for Natural Language Inference (NLI) using nli-roberta-base\n",
    "#  inference = pipeline('zero-shot-classification', model='roberta-large-mnli', device=0)\n",
    "# Prepare Name Entity Recognition (NER)\n",
    "  ner = pipeline(\"token-classification\", model=\"Jean-Baptiste/roberta-large-ner-english\", aggregation_strategy=\"simple\", device=0)\n",
    "else:\n",
    "# Prepare a pipeline for sentiment-analysis\n",
    "  classifier = pipeline(\"text-classification\", model=\"j-hartmann/sentiment-roberta-large-english-3-classes\")\n",
    "# Prepare for Natural Language Inference (NLI) using nli-roberta-base\n",
    "#  inference = pipeline('zero-shot-classification', model='roberta-large-mnli')\n",
    "# Prepare Name Entity Recognition (NER)\n",
    "  ner = pipeline(\"token-classification\", model=\"Jean-Baptiste/roberta-large-ner-english\", aggregation_strategy=\"simple\")\n",
    "inference_labels = ['entailment', 'neutral', 'contradiction']\n",
    "\n",
    "# Load en_core_sci_scibert model to separate sentences\n",
    "nlp = spacy.load('en_core_sci_scibert')\n",
    "\n",
    "\n",
    "# Define input and output folders\n",
    "input_folder = './texts'\n",
    "output_folder = \"./jsonFiles\"\n",
    "#Initiallize lists and dictionaries for graphs and paper data\n",
    "graphs = []\n",
    "# For paper identification\n",
    "paper_data = {}\n",
    "\n",
    "#Regex necessary for identifying citations in text\n",
    "author = \"(?:[A-Z][A-Za-z'`-]+)\"\n",
    "etal = \"(?:et al.?)\"\n",
    "additional = \"(?:,? (?:(?:and |& )?\" + author + \"|\" + etal + \"))\"\n",
    "year_num = \"(?:19|20)[0-9][0-9]\"\n",
    "page_num = \"(?:, p.? [0-9]+)?\"\n",
    "year = \"(?:,? *\" + year_num + page_num + \"| *\\(\" + year_num + page_num + \"\\))\"\n",
    "name_year_regex = \"(\" + author + additional + \"*\" + year + \")\"\n",
    "#In case the regex type above is not used we assume the regex might look like this: [1]\n",
    "num_bracket_regex = r\"(\\[\\d+\\])\"\n",
    "regex = name_year_regex + \"|\" + num_bracket_regex\n",
    "\n",
    "# Start of json to node and edge creation\n",
    "def get_cited_papers(paper_id, sentence):\n",
    "  using_et_al = False\n",
    "  if '[' in sentence:\n",
    "    citations = re.findall(num_bracket_regex, sentence)\n",
    "  else:\n",
    "    citations = re.findall(name_year_regex, sentence)\n",
    "    using_et_al = True\n",
    "  global_ids = []\n",
    "\n",
    "  for citation in citations:\n",
    "    if using_et_al:\n",
    "      cit_id = citation\n",
    "    else:\n",
    "      cit_id = citation[1].replace('[','').replace(']','')\n",
    "    global_ids.append(manipulate_data.append_reference_data(paper_data, paper_id, cit_id))\n",
    "\n",
    "  return global_ids\n",
    "\n",
    "def id_sentiment_id(citer, sentences):\n",
    "  entities_relations = []\n",
    "\n",
    "  for sentence in sentences:\n",
    "    score = sentence[\"final_score\"]\n",
    "    cited_papers = sentence[\"0\"][2]\n",
    "    for cited in cited_papers:\n",
    "      topic = ner(sentence[\"0\"][0])\n",
    "      citation_relations = [citer, score, cited, topic]\n",
    "      entities_relations.append(citation_relations)\n",
    "\n",
    "  return entities_relations\n",
    "# End of json to node and edge creation\n",
    "\n",
    "\n",
    "# Start of KG generation\n",
    "def gradient_color(mapped_value):\n",
    "    # Define the color points and colors for the gradient\n",
    "    color_points = [-1, -0.1, 0]\n",
    "    colors = ['#9B3131', '#90e0ef', '#1D9A60']\n",
    "    color = ''\n",
    "\n",
    "    for i in range(3):\n",
    "      if mapped_value > color_points[i]:\n",
    "        color = colors[i]\n",
    "\n",
    "    return color\n",
    "\n",
    "# get the label to put on the node based on the global ID\n",
    "def get_citation_from_global_id(global_id):\n",
    "  if isinstance(global_id, int):\n",
    "    author = paper_data[global_id][\"Author\"]\n",
    "    title = paper_data[global_id][\"Title\"]\n",
    "    year = paper_data[global_id][\"Year\"]\n",
    "    node = f\"{author}, {year}\"\n",
    "    if len(title) == 0:\n",
    "      title = \"Title not found!\"\n",
    "    return node, title\n",
    "  for paper in paper_data:\n",
    "    if paper == global_id:\n",
    "      author = paper_data[paper][\"Author\"]\n",
    "      title = paper_data[paper][\"Title\"]\n",
    "      year = paper_data[paper][\"Year\"]\n",
    "      node = f\"{author}, {year}\"\n",
    "      if len(title) == 0:\n",
    "        title = \"Title not found!\"\n",
    "      return node, title\n",
    "  return global_id, \"\"\n",
    "\n",
    "# get the global_ID number/code based on the citation ID, specific to the citing papers\n",
    "def get_ref_code(citation_id):\n",
    "  for paper in paper_data:\n",
    "    if paper == citation_id:\n",
    "      return paper\n",
    "\n",
    "  return len(paper_data)\n",
    "\n",
    "# After preparing all elements for the graph based on each paper specific graph, generate the nodes and edges accordingly\n",
    "def visualize_KG(html_name):\n",
    "  g=net.Network(notebook=True, cdn_resources='in_line')#, directed=True)\n",
    "\n",
    "  for graph in graphs:\n",
    "    for nodes_rel in graph:\n",
    "      citer = nodes_rel[0]\n",
    "      cited, cited_title = get_citation_from_global_id(nodes_rel[2])\n",
    "      topics = nodes_rel[3]\n",
    "      edge_label = \"\"\n",
    "      for topic in topics:\n",
    "        if not topic['entity_group'] == 'PER':\n",
    "          new_topic = topic['word']\n",
    "          if len(edge_label) == 0:\n",
    "            edge_label = new_topic\n",
    "          else:\n",
    "            edge_label = f\"{edge_label}, {new_topic}\"\n",
    "\n",
    "\n",
    "      rel_color = gradient_color(nodes_rel[1])\n",
    "      edge_width = abs(nodes_rel[1]) + 0.5\n",
    "\n",
    "      citer_ref = get_ref_code(citer)\n",
    "      cited_ref = nodes_rel[2]\n",
    "\n",
    "      citer, citer_title = get_citation_from_global_id(citer)\n",
    "\n",
    "      if ', 0' in cited:\n",
    "        continue\n",
    "\n",
    "      g.add_node(citer_ref, label = citer, title = citer_title)\n",
    "      g.add_node(cited_ref, label = cited, color = rel_color, title = cited_title)\n",
    "      if len(edge_label) == 0:\n",
    "        edge_label = 'No Named Entity found!'\n",
    "      g.add_edge(citer_ref, cited_ref, color = rel_color, width = (edge_width*edge_width), title = edge_label)\n",
    "\n",
    "  #g.show(html_name)\n",
    "  html = g.generate_html()\n",
    "  with open(html_name, mode='w', encoding='utf-8') as fp:\n",
    "    fp.write(html)\n",
    "  display(HTML(html))\n",
    "# End of KG generation\n",
    "\n",
    "# For each document get the id of the reference papers to be used as entities\n",
    "def prepare_entities(documents):\n",
    "  for doc in documents:\n",
    "    # Read the JSON data from the file\n",
    "    with open(doc, 'r') as json_file:\n",
    "      data = json.load(json_file)\n",
    "\n",
    "    id = data[\"id\"]\n",
    "    sentences = data[\"sentences\"]\n",
    "\n",
    "    graph = id_sentiment_id(id, sentences)\n",
    "    graphs.append(graph)\n",
    "\n",
    "# Function that gets the documents availble in a folder\n",
    "def get_docs(documents_folder):\n",
    "    files = []\n",
    "    for file_name in os.listdir(documents_folder):\n",
    "        file_path = os.path.join(documents_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            files.append(file_path)\n",
    "    return files\n",
    "\n",
    "# load if only KG is needed to be generated\n",
    "def load_paper_data(file_path):\n",
    "  with open(file_path, 'r') as json_file:\n",
    "    paper_data = json.load(json_file)\n",
    "\n",
    "# Check if a given text contains a citation\n",
    "def has_citation(text):\n",
    "    pattern = re.compile(regex)\n",
    "    match = pattern.search(text)\n",
    "    return match is not None\n",
    "\n",
    "# Change the result of the Sent. Analysis so as to have the score go from ]-1;1[\n",
    "def format_sentiment_result(sen):\n",
    "    data = classifier(sen.text)[0]\n",
    "    sentiment = data[\"label\"]\n",
    "    value = data[\"score\"]\n",
    "\n",
    "    if sentiment == 'negative':\n",
    "      value = -value\n",
    "    elif sentiment == 'neutral':\n",
    "      value = 0\n",
    "\n",
    "    return value\n",
    "\n",
    "def get_context_from_noun_phrases(sen1, sen2):\n",
    "    doc = nlp(sen2)\n",
    "    print(sen2)\n",
    "    for token2 in doc:\n",
    "        print(token2.text + \"--> \" + token2.head.text) \n",
    "        if token2.head.text in sen1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_context_from_similarity(sen1, sen2):\n",
    "  doc1 = nlp(sen1)\n",
    "  doc2 = nlp(sen2)\n",
    "\n",
    "  similarity = doc1.similarity(doc2)\n",
    "  return similarity\n",
    "\n",
    "def prepare_citation_sections(paper_id, sens):\n",
    "  prev_sen = None\n",
    "  prev_context_value = next_context_value = 0\n",
    "  citation_json = []\n",
    "\n",
    "  for sen, next_sen in zip(sens, sens[1:] + [None]):\n",
    "    if has_citation(sen.text):\n",
    "      sen_value = format_sentiment_result(sen)\n",
    "      cited_papers = get_cited_papers(paper_id, sen.text)\n",
    "      citation_section = {}\n",
    "      #named_entities = ner(sen.text)\n",
    "      noun_phrases = nlp(sen.text).noun_chunks\n",
    "\n",
    "      if prev_sen and not has_citation(prev_sen.text):\n",
    "        print(\"taken into consideration\")\n",
    "        #context = get_context(prev_sen.text, sen.text)\n",
    "        #context_found = get_context_from_ner(prev_sen.text, named_entities)\n",
    "        context_found = get_context_from_noun_phrases(prev_sen.text, sen.text)\n",
    "        #similarity = get_context_from_similarity(prev_sen.text, sen.text)\n",
    "        prev_context_value = 0\n",
    "        #if not context == 'neutral':\n",
    "        if context_found:\n",
    "          #prev_context_value = get_contextual_result(prev_sen, '', True)\n",
    "          prev_context_value = format_sentiment_result(prev_sen)\n",
    "          citation_section[-1] = [prev_sen.text, prev_context_value]\n",
    "\n",
    "      citation_section[0] = [sen.text, sen_value, cited_papers]\n",
    "\n",
    "      if next_sen and not has_citation(next_sen.text):\n",
    "        #context = get_context(sen.text, next_sen.text)\n",
    "        #context_found = get_context_from_ner(next_sen.text, named_entities)\n",
    "        context_found = get_context_from_noun_phrases(sen.text, next_sen.text)\n",
    "        #similarity = get_context_from_similarity(sen.text, next_sen.text)\n",
    "        next_context_value = 0\n",
    "        #if not context == 'neutral':\n",
    "        if context_found:\n",
    "          #next_context_value = get_contextual_result(next_sen, '', False)\n",
    "          next_context_value = format_sentiment_result(next_sen)\n",
    "          citation_section[1] = [next_sen.text, next_context_value]\n",
    "    \n",
    "      citation_section[\"final_score\"] = sen_value + prev_context_value + next_context_value\n",
    "      citation_json.append(citation_section)\n",
    "\n",
    "    prev_sen = sen\n",
    "\n",
    "  return citation_json\n",
    "\n",
    "\n",
    "def text_to_json(paper_id, text, nlp):\n",
    "    doc = nlp(text)\n",
    "    sens = list(doc.sents)\n",
    "\n",
    "    citation_json = prepare_citation_sections(paper_id, sens)\n",
    "\n",
    "    return citation_json\n",
    "\n",
    "\n",
    "def extract_text_from_file(text_file):\n",
    "    print(f\"Reading {text_file}\")\n",
    "    title = ''\n",
    "\n",
    "    with open(text_file, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    if 'Title: ' in text:\n",
    "      title = text.split('\\n')[0].replace('Title: ', '')\n",
    "      text = text.split(\"\\n\",1)[1].replace('\\n', ' ')\n",
    "\n",
    "    return text, title\n",
    "\n",
    "\n",
    "def save_citation_sections(text_file, json_dir, nlp):\n",
    "    text, paper_id = extract_citing_data(text_file)\n",
    "\n",
    "    citation_json = text_to_json(paper_id, text, nlp)\n",
    "\n",
    "    json_data = {\n",
    "        \"id\": paper_id,\n",
    "        \"sentences\": citation_json\n",
    "    }\n",
    "\n",
    "    file_path = os.path.join(json_dir, paper_id+\".json\")\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(json_data, file, indent=4)\n",
    "\n",
    "    print(f\"  Citation sections written to {file_path}\")\n",
    "\n",
    "\n",
    "def get_citing_author_year(citing_paper_id):\n",
    "  publication_year = re.search(r\"\\d{4}$\", citing_paper_id).group()\n",
    "\n",
    "  author =  citing_paper_id.replace(publication_year, '')\n",
    "\n",
    "  author = re.sub(\"EtAl$\", '', author, flags=re.IGNORECASE).replace(r\".*And.*\", \" and \")\n",
    "\n",
    "  return author, publication_year\n",
    "\n",
    "\n",
    "def extract_citing_data(text_file):\n",
    "  text, title = extract_text_from_file(text_file)\n",
    "  paper_id = os.path.basename(text_file).replace(\".txt\", \"\")\n",
    "  author, publication_year = get_citing_author_year(paper_id)\n",
    "\n",
    "  paper_data[paper_id] = {\n",
    "      \"Author\": author,\n",
    "      \"Title\": title,\n",
    "      \"Year\": publication_year\n",
    "  }\n",
    "\n",
    "  return text, paper_id\n",
    "\n",
    "\n",
    "only_kg_generation = False\n",
    "if not only_kg_generation:\n",
    "  docs = get_docs(input_folder)\n",
    "  sorted_docs = sorted(docs)\n",
    "  cnt = 1\n",
    "  if os.path.exists(output_folder):\n",
    "    while os.path.exists(f\"{output_folder}_({cnt})\"):\n",
    "      cnt += 1\n",
    "    output_folder = output_folder + f\"_({cnt})\"\n",
    "\n",
    "  os.makedirs(output_folder)\n",
    "\n",
    "  for doc in sorted_docs:\n",
    "    if not 'bibliography' in doc:\n",
    "      print(\"_________________________________________________________________________________________________________________________\")\n",
    "      save_citation_sections(doc, output_folder, nlp)\n",
    "\n",
    "print(\"Citations extracted!\")\n",
    "\n",
    "if only_kg_generation:\n",
    "  output_folder = \"./jsonFiles\"\n",
    "  load_paper_data(\"./citations.json\")\n",
    "\n",
    "json_docs = get_docs(output_folder)\n",
    "prepare_entities(json_docs)\n",
    "\n",
    "if not only_kg_generation:\n",
    "  with open('citations.json', \"w\") as file:\n",
    "    json.dump(paper_data, file, indent=2)\n",
    "\n",
    "visualize_KG('WIP_KG.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
