{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a82d3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install json\n",
    "!pip install spacy\n",
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_scibert-0.5.1.tar.gz\n",
    "!pip install transformers\n",
    "!pip install pyvis\n",
    "!pip install spacy-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "848aea9b-277e-4ec7-bba4-16ea3e15a227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macac\\miniconda3\\envs\\Cit2KG\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________________________________________________________________\n",
      "Reading ./texts\\AburaedEtAl2017.txt\n",
      "Citation sections written to ./jsonFiles_(1)\\AburaedEtAl2017.json\n",
      "_________________________________________________________________________________________________________________________\n",
      "Reading ./texts\\AburaedEtAl2018.txt\n",
      "Citation sections written to ./jsonFiles_(1)\\AburaedEtAl2018.json\n",
      "_________________________________________________________________________________________________________________________\n",
      "Reading ./texts\\AburaedEtAl2020.txt\n",
      "Citation sections written to ./jsonFiles_(1)\\AburaedEtAl2020.json\n",
      "_________________________________________________________________________________________________________________________\n",
      "Reading ./texts\\BosselutEtAl2019.txt\n",
      "Citation sections written to ./jsonFiles_(1)\\BosselutEtAl2019.json\n",
      "_________________________________________________________________________________________________________________________\n",
      "Reading ./texts\\ChenEtAl2019.txt\n",
      "Citation sections written to ./jsonFiles_(1)\\ChenEtAl2019.json\n",
      "_________________________________________________________________________________________________________________________\n",
      "Reading ./texts\\HoangAndKan2020.txt\n",
      "Citation sections written to ./jsonFiles_(1)\\HoangAndKan2020.json\n",
      "_________________________________________________________________________________________________________________________\n",
      "Reading ./texts\\KaplanEtAl2009.txt\n",
      "Citation sections written to ./jsonFiles_(1)\\KaplanEtAl2009.json\n",
      "_________________________________________________________________________________________________________________________\n",
      "Reading ./texts\\KertkeidkachornIchise2017.txt\n",
      "Citation sections written to ./jsonFiles_(1)\\KertkeidkachornIchise2017.json\n",
      "_________________________________________________________________________________________________________________________\n",
      "Reading ./texts\\Liu2017.txt\n",
      "Citation sections written to ./jsonFiles_(1)\\Liu2017.json\n",
      "_________________________________________________________________________________________________________________________\n",
      "Reading ./texts\\LuanEtAl2018.txt\n",
      "Citation sections written to ./jsonFiles_(1)\\LuanEtAl2018.json\n",
      "_________________________________________________________________________________________________________________________\n",
      "Reading ./texts\\LuanEtAl2019.txt\n",
      "Citation sections written to ./jsonFiles_(1)\\LuanEtAl2019.json\n",
      "_________________________________________________________________________________________________________________________\n",
      "Reading ./texts\\MaEtAl2016.txt\n",
      "Citation sections written to ./jsonFiles_(1)\\MaEtAl2016.json\n",
      "_________________________________________________________________________________________________________________________\n",
      "Reading ./texts\\MartinezRodriguezEtAl2018.txt\n",
      "Citation sections written to ./jsonFiles_(1)\\MartinezRodriguezEtAl2018.json\n",
      "_________________________________________________________________________________________________________________________\n",
      "Reading ./texts\\MelnykEtAl2022.txt\n",
      "Citation sections written to ./jsonFiles_(1)\\MelnykEtAl2022.json\n",
      "_________________________________________________________________________________________________________________________\n",
      "Reading ./texts\\WaddenEtAl2019.txt\n",
      "Citation sections written to ./jsonFiles_(1)\\WaddenEtAl2019.json\n",
      "_________________________________________________________________________________________________________________________\n",
      "Reading ./texts\\WangEtAl2022.txt\n",
      "Citation sections written to ./jsonFiles_(1)\\WangEtAl2022.json\n",
      "Citations extracted!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sys import stdin\n",
    "import os\n",
    "import json\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "# Allocate a pipeline for sentiment-analysis\n",
    "classifier = pipeline(\"text-classification\", model=\"j-hartmann/sentiment-roberta-large-english-3-classes\")\n",
    "#second_opinion = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# Prepare for inference using nli-roberta-base\n",
    "inference = pipeline('zero-shot-classification', model='roberta-large-mnli')\n",
    "inference_labels = ['entailment', 'neutral', 'contradiction']\n",
    "\n",
    "#folder containing the text files\n",
    "documents_folder = './texts'\n",
    "\n",
    "#Regex necessary for identifying citations in text\n",
    "author = \"(?:[A-Z][A-Za-z'`-]+)\"\n",
    "etal = \"(?:et al.?)\"\n",
    "additional = \"(?:,? (?:(?:and |& )?\" + author + \"|\" + etal + \"))\"\n",
    "year_num = \"(?:19|20)[0-9][0-9]\"\n",
    "page_num = \"(?:, p.? [0-9]+)?\"\n",
    "year = \"(?:,? *\" + year_num + page_num + \"| *\\(\" + year_num + page_num + \"\\))\"\n",
    "name_year_regex = \"(\" + author + additional + \"*\" + year + \")\"\n",
    "\n",
    "#In case the regex type above is not used we assume the regex might look like this: [1]\n",
    "num_bracket_regex = r\"(?:\\[\\d+\\])\"\n",
    "\n",
    "def get_docs(documents_folder):\n",
    "    files = []\n",
    "    for file_name in os.listdir(documents_folder):\n",
    "        file_path = os.path.join(documents_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            files.append(file_path)\n",
    "    return files\n",
    "\n",
    "\n",
    "def check_if_sentence_matches(text, regex):\n",
    "    pattern = re.compile(regex)\n",
    "    match = pattern.search(text)\n",
    "    return match is not None\n",
    "\n",
    "\n",
    "def extract_text_from_file(text_file):\n",
    "    print(f\"Reading {text_file}\")\n",
    "\n",
    "    with open(text_file, 'r') as file:\n",
    "        text = file.read().replace('\\n', ' ')\n",
    "\n",
    "    return text\n",
    "\n",
    "# Consider removing\n",
    "#def check_if_neutral(sen, original_value):\n",
    "#    sentiment = second_opinion(sen)[0]\n",
    "#\n",
    "#    rate = sentiment['label'][0]\n",
    "#\n",
    "#    modifier = 0\n",
    "#\n",
    "#    for i in range(1,6):\n",
    "#      if str(i) == rate:\n",
    "#        modifier = -1 + (0.4 * i) + ((1- sentiment['score']) * (-0.4))\n",
    "#\n",
    "#    return (1 - original_value) * modifier\n",
    "\n",
    "\n",
    "def mapping_function(sen, sentiment, value):\n",
    "    mapped_value = value\n",
    "\n",
    "    if sentiment == 'negative':\n",
    "      mapped_value = -value\n",
    "\n",
    "    if sentiment == 'neutral':\n",
    "      mapped_value = 0\n",
    "\n",
    "    return mapped_value\n",
    "\n",
    "\n",
    "def format_sentiment_result(sen):\n",
    "    data = classifier(sen.text)[0]\n",
    "\n",
    "    return mapping_function(sen.text, data[\"label\"], data[\"score\"])\n",
    "\n",
    "def inference_calculation(sen, inferring_sen):\n",
    "  sequence_to_classify = sen + \" \" + inferring_sen\n",
    "\n",
    "  result = inference(sequence_to_classify, inference_labels)\n",
    "  print(result)\n",
    "  labels = result['labels']\n",
    "\n",
    "  return labels[0]\n",
    "\n",
    "def get_score_modifier(inference_result):\n",
    "    if inference_result == 'entailment' or inference_result == 'contradiction':\n",
    "      return 1\n",
    "\n",
    "    return 0\n",
    "\n",
    "def get_modified_score(citation_section, sen_num, cit_sen):\n",
    "  score = citation_section[sen_num][1]\n",
    "  if abs(score) < 0.2:\n",
    "    return 0\n",
    "  if sen_num == -1:\n",
    "    inference_result = inference_calculation(citation_section[sen_num][0], cit_sen)\n",
    "  if sen_num == 1:\n",
    "    inference_result = inference_calculation(cit_sen, citation_section[sen_num][0])\n",
    "\n",
    "  return get_score_modifier(inference_result) * score\n",
    "\n",
    "def calculate_score(citation_section):\n",
    "  cit_sen = citation_section[0][0]\n",
    "  cit_score = citation_section[0][1]\n",
    "  prev_score = 0\n",
    "  next_score = 0\n",
    "\n",
    "  if -1 in citation_section:\n",
    "    prev_score = get_modified_score(citation_section, -1, cit_sen)\n",
    "\n",
    "  if 1 in citation_section:\n",
    "    next_score = get_modified_score(citation_section, 1, cit_sen)\n",
    "\n",
    "  return cit_score + prev_score + next_score\n",
    "\n",
    "\n",
    "def analyze_citation_sentences(sens, regex):\n",
    "    prev_sen = None\n",
    "    citation_json = []\n",
    "    #citation_text = \"\"\n",
    "\n",
    "    for sen, next_sen in zip(sens, sens[1:] + [None]):\n",
    "        if check_if_sentence_matches(sen.text, regex):\n",
    "            sen_value = format_sentiment_result(sen);\n",
    "            citation_section = {\n",
    "                0: [sen.text , sen_value]\n",
    "            }\n",
    "\n",
    "            if prev_sen and not check_if_sentence_matches(prev_sen.text, regex):\n",
    "                prev_value = format_sentiment_result(prev_sen)\n",
    "                #citation_text = citation_text + \" \" + prev_sen.text\n",
    "                citation_section = {\n",
    "                  -1: [prev_sen.text, prev_value],\n",
    "                  0: [sen.text, sen_value]\n",
    "                }\n",
    "\n",
    "                #citation_text = citation_text + \" \" + sen.text\n",
    "\n",
    "            if next_sen and not check_if_sentence_matches(next_sen.text, regex):\n",
    "                next_value = format_sentiment_result(next_sen)\n",
    "                #citation_text = citation_text + \" \" + next_sen.text\n",
    "                citation_section[1] = [next_sen.text, next_value]\n",
    "\n",
    "            citation_section[\"final_score\"] = calculate_score(citation_section)\n",
    "            citation_json.append(citation_section)\n",
    "\n",
    "        prev_sen = sen\n",
    "\n",
    "    return citation_json  #, citation_text\n",
    "\n",
    "def extract_citation_sections(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    sens = list(doc.sents)\n",
    "\n",
    "    regex = name_year_regex + \"|\" + num_bracket_regex\n",
    "\n",
    "    citation_json = analyze_citation_sentences(sens, regex)#, citation_text = analyze_citation_sentences(sens, regex)\n",
    "\n",
    "    return citation_json  #, citation_text\n",
    "\n",
    "\n",
    "def save_citation_sections(text_file, json_dir, nlp):\n",
    "    text = extract_text_from_file(text_file)\n",
    "    paperId = os.path.basename(text_file).replace(\".txt\", \"\")\n",
    "\n",
    "    citation_json = extract_citation_sections(text, nlp) #, citation_text = extract_citation_sections(text, nlp)\n",
    "\n",
    "    json_data = {\n",
    "        \"id\": paperId,\n",
    "        \"sentences\": citation_json\n",
    "    }\n",
    "\n",
    "    #text_path = os.path.join(json_dir, paperId+\"-citations.txt\")\n",
    "    file_path = os.path.join(json_dir, paperId+\".json\")\n",
    "\n",
    "    #with open(text_path, \"w\") as file:\n",
    "    #    file.write(citation_text)\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(json_data, file, indent=4)\n",
    "\n",
    "    print(f\"Citation sections written to {file_path}\")\n",
    "\n",
    "\n",
    "docs = get_docs(documents_folder)\n",
    "nlp = spacy.load('en_core_sci_scibert')\n",
    "original_dir_name = \"./jsonFiles\"\n",
    "cnt = 1\n",
    "if os.path.exists(original_dir_name):\n",
    "    while os.path.exists(f\"{original_dir_name}_({cnt})\"):\n",
    "        cnt += 1\n",
    "    original_dir_name = original_dir_name + f\"_({cnt})\"\n",
    "\n",
    "os.makedirs(original_dir_name)\n",
    "\n",
    "for doc in docs:\n",
    "    print(\"_________________________________________________________________________________________________________________________\")\n",
    "    save_citation_sections(doc, original_dir_name, nlp)\n",
    "\n",
    "print(\"Citations extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2601874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Regex necessary for identifying citations in text\n",
    "author = \"(?:[A-Z][A-Za-z'`-]+)\"\n",
    "etal = \"(?:et al.?)\"\n",
    "additional = \"(?:,? (?:(?:and |& )?\" + author + \"|\" + etal + \"))\"\n",
    "year_num = \"(?:19|20)[0-9][0-9]\"\n",
    "page_num = \"(?:, p.? [0-9]+)?\"\n",
    "year = \"(?:,? *\" + year_num + page_num + \"| *\\(\" + year_num + page_num + \"\\))\"\n",
    "name_year_regex = \"(\" + author + additional + \"*\" + year + \")\"\n",
    "\n",
    "# In case the regex type above is not used we assume the regex might look like this: [1]\n",
    "num_bracket_regex = r\"(?:\\[\\d+\\])\"\n",
    "\n",
    "regex = name_year_regex + \"|\" + num_bracket_regex\n",
    "\n",
    "documents_folder= \"./jsonFiles\"\n",
    "\n",
    "docs = get_docs(documents_folder)\n",
    "\n",
    "def get_cited_papers(sentence):\n",
    "  return re.findall(regex, sentence)\n",
    "\n",
    "def id_sentiment_id(citer, sentences):\n",
    "  entities_relations = []\n",
    "\n",
    "  for sentence in sentences:\n",
    "    score = sentence[\"final_score\"]\n",
    "    citation_sen = sentence[\"0\"]\n",
    "    for cited in get_cited_papers(citation_sen[0]):\n",
    "      citation_relations = [citer, score, cited]\n",
    "      entities_relations.append(citation_relations)\n",
    "\n",
    "  return entities_relations\n",
    "\n",
    "graphs = []\n",
    "\n",
    "for doc in docs:\n",
    "  # Read the JSON data from the file\n",
    "  with open(doc, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "  id = data[\"id\"]\n",
    "  sentences = data[\"sentences\"]\n",
    "  print(f\"Reading: {id}\")\n",
    "\n",
    "  graph = id_sentiment_id(id, sentences)\n",
    "  graphs.append(graph)\n",
    "\n",
    "  print(graph)\n",
    "  print(\"___________________________________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10010c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvis\n",
    "from pyvis import network as net\n",
    "import networkx as nx\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def gradient_color(mapped_value):\n",
    "    # Define the color points and colors for the gradient\n",
    "    color_points = [-1, -0.5, 0.5]\n",
    "    colors = ['#9B3131', '#3F7190', '#1D9A60']\n",
    "    color = ''\n",
    "\n",
    "    for i in range(3):\n",
    "      if mapped_value > color_points[i]:\n",
    "        color = colors[i]\n",
    "\n",
    "    return color\n",
    "\n",
    "g=net.Network(notebook=True, cdn_resources='in_line')#, directed=True)\n",
    "\n",
    "node_list = []\n",
    "rel_dict = []\n",
    "\n",
    "for graph in graphs:\n",
    "  for nodes_rel in graph:\n",
    "    citer = nodes_rel[0]\n",
    "    cited = re.sub(r'[ ,.()]', '', nodes_rel[2]).replace('et','Et').replace('al','Al')\n",
    "    if not citer in node_list:\n",
    "      node_list.append(citer)\n",
    "    if not cited in node_list:\n",
    "      node_list.append(cited)\n",
    "\n",
    "    rel_color = gradient_color(nodes_rel[1])\n",
    "    edge_width = abs(nodes_rel[1]) + 0.5\n",
    "\n",
    "    citer_ref = node_list.index(citer)\n",
    "    cited_ref = node_list.index(cited)\n",
    "\n",
    "    g.add_node(citer_ref, label = citer)\n",
    "    g.add_node(cited_ref, label = cited, color = rel_color)\n",
    "    g.add_edge(citer_ref, cited_ref, color = rel_color, width = (edge_width*edge_width))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "g.show(\"WIP_KG.html\")\n",
    "display(HTML('WIP_KG.html'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
