{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a82d3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install json\n",
    "!pip install spacy\n",
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_scibert-0.5.1.tar.gz\n",
    "!pip install transformers\n",
    "!pip install pyvis\n",
    "!pip install spacy-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848aea9b-277e-4ec7-bba4-16ea3e15a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from sys import stdin\n",
    "import os\n",
    "import json\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import spacy\n",
    "import pyvis\n",
    "from pyvis import network as net\n",
    "import networkx as nx\n",
    "from IPython.core.display import display, HTML\n",
    "sys.path.append(\"code\")\n",
    "import citation_data_manipulation as manipulate_data\n",
    "\n",
    "# Check if gpu is available\n",
    "gpu_available = torch.cuda.is_available()\n",
    "\n",
    "# Prepare various pipelines based on whether gpu is available\n",
    "if gpu_available:\n",
    "  print(\"Using GPU\")\n",
    "# Prepare a pipeline for sentiment-analysis\n",
    "  classifier = pipeline(\"text-classification\", model=\"j-hartmann/sentiment-roberta-large-english-3-classes\", device=0)\n",
    "# Prepare for Natural Language Inference (NLI) using nli-roberta-base\n",
    "  inference = pipeline('zero-shot-classification', model='roberta-large-mnli', device=0)\n",
    "# Prepare Name Entity Recognition (NER)\n",
    "  ner = pipeline(\"token-classification\", model=\"Jean-Baptiste/roberta-large-ner-english\", aggregation_strategy=\"simple\", device=0)\n",
    "else:\n",
    "# Prepare a pipeline for sentiment-analysis\n",
    "  classifier = pipeline(\"text-classification\", model=\"j-hartmann/sentiment-roberta-large-english-3-classes\")\n",
    "# Prepare for Natural Language Inference (NLI) using nli-roberta-base\n",
    "  inference = pipeline('zero-shot-classification', model='roberta-large-mnli')\n",
    "# Prepare Name Entity Recognition (NER)\n",
    "  ner = pipeline(\"token-classification\", model=\"Jean-Baptiste/roberta-large-ner-english\", aggregation_strategy=\"simple\")\n",
    "inference_labels = ['entailment', 'neutral', 'contradiction']\n",
    "\n",
    "# Load en_core_sci_scibert model to separate sentences\n",
    "nlp = spacy.load('en_core_sci_scibert')\n",
    "\n",
    "\n",
    "# Define input and output folders\n",
    "input_folder = './texts'\n",
    "output_folder = \"./jsonFiles\"\n",
    "#Initiallize lists and dictionaries for graphs and paper data\n",
    "graphs = []\n",
    "# For paper identification\n",
    "paper_data = {}\n",
    "\n",
    "#Regex necessary for identifying citations in text\n",
    "author = \"(?:[A-Z][A-Za-z'`-]+)\"\n",
    "etal = \"(?:et al.?)\"\n",
    "additional = \"(?:,? (?:(?:and |& )?\" + author + \"|\" + etal + \"))\"\n",
    "year_num = \"(?:19|20)[0-9][0-9]\"\n",
    "page_num = \"(?:, p.? [0-9]+)?\"\n",
    "year = \"(?:,? *\" + year_num + page_num + \"| *\\(\" + year_num + page_num + \"\\))\"\n",
    "name_year_regex = \"(\" + author + additional + \"*\" + year + \")\"\n",
    "#In case the regex type above is not used we assume the regex might look like this: [1]\n",
    "num_bracket_regex = r\"(\\[\\d+\\])\"\n",
    "regex = name_year_regex + \"|\" + num_bracket_regex\n",
    "\n",
    "# Start of json to node and edge creation\n",
    "def get_cited_papers(paper_id, sentence):\n",
    "  using_et_al = False\n",
    "  if '[' in sentence:\n",
    "    citations = re.findall(num_bracket_regex, sentence)\n",
    "  else:\n",
    "    citations = re.findall(name_year_regex, sentence)\n",
    "    using_et_al = True\n",
    "  global_ids = []\n",
    "\n",
    "  for citation in citations:\n",
    "    if using_et_al:\n",
    "      cit_id = citation\n",
    "    else:\n",
    "      cit_id = citation[1].replace('[','').replace(']','')\n",
    "    global_ids.append(manipulate_data.append_reference_data(paper_data, paper_id, cit_id))\n",
    "\n",
    "  return global_ids\n",
    "\n",
    "def id_sentiment_id(citer, sentences):\n",
    "  entities_relations = []\n",
    "\n",
    "  for sentence in sentences:\n",
    "    score = sentence[\"final_score\"]\n",
    "    cited_papers = sentence[\"0\"][2]\n",
    "    for cited in cited_papers:\n",
    "      topic = ner(sentence[\"0\"][0])\n",
    "      citation_relations = [citer, score, cited, topic]\n",
    "      entities_relations.append(citation_relations)\n",
    "\n",
    "  return entities_relations\n",
    "# End of json to node and edge creation\n",
    "\n",
    "\n",
    "# Start of KG generation\n",
    "def gradient_color(mapped_value):\n",
    "    # Define the color points and colors for the gradient\n",
    "    color_points = [-1, -0.1, 0]\n",
    "    colors = ['#9B3131', '#90e0ef', '#1D9A60']\n",
    "    color = ''\n",
    "\n",
    "    for i in range(3):\n",
    "      if mapped_value > color_points[i]:\n",
    "        color = colors[i]\n",
    "\n",
    "    return color\n",
    "\n",
    "# get the label to put on the node based on the global ID\n",
    "def get_citation_from_global_id(global_id):\n",
    "  if isinstance(global_id, int):\n",
    "    author = paper_data[global_id][\"Author\"]\n",
    "    title = paper_data[global_id][\"Title\"]\n",
    "    year = paper_data[global_id][\"Year\"]\n",
    "    node = f\"{author}, {year}\"\n",
    "    if len(title) == 0:\n",
    "      title = \"Title not found!\"\n",
    "    return node, title\n",
    "  for paper in paper_data:\n",
    "    if paper == global_id:\n",
    "      author = paper_data[paper][\"Author\"]\n",
    "      title = paper_data[paper][\"Title\"]\n",
    "      year = paper_data[paper][\"Year\"]\n",
    "      node = f\"{author}, {year}\"\n",
    "      if len(title) == 0:\n",
    "        title = \"Title not found!\"\n",
    "      return node, title\n",
    "  return global_id, \"\"\n",
    "\n",
    "# get the global_ID number/code based on the citation ID, specific to the citing papers\n",
    "def get_ref_code(citation_id):\n",
    "  for paper in paper_data:\n",
    "    if paper == citation_id:\n",
    "      return paper\n",
    "\n",
    "  return len(paper_data)\n",
    "\n",
    "# After preparing all elements for the graph based on each paper specific graph, generate the nodes and edges accordingly\n",
    "def visualize_KG(html_name):\n",
    "  g=net.Network(notebook=True, cdn_resources='in_line')#, directed=True)\n",
    "\n",
    "  for graph in graphs:\n",
    "    for nodes_rel in graph:\n",
    "      citer = nodes_rel[0]\n",
    "      cited, cited_title = get_citation_from_global_id(nodes_rel[2])\n",
    "      topics = nodes_rel[3]\n",
    "      edge_label = \"\"\n",
    "      for topic in topics:\n",
    "        if not topic['entity_group'] == 'PER':\n",
    "          new_topic = topic['word']\n",
    "          if len(edge_label) == 0:\n",
    "            edge_label = new_topic\n",
    "          else:\n",
    "            edge_label = f\"{edge_label}, {new_topic}\"\n",
    "\n",
    "\n",
    "      rel_color = gradient_color(nodes_rel[1])\n",
    "      edge_width = abs(nodes_rel[1]) + 0.5\n",
    "\n",
    "      citer_ref = get_ref_code(citer)\n",
    "      cited_ref = nodes_rel[2]\n",
    "\n",
    "      citer, citer_title = get_citation_from_global_id(citer)\n",
    "\n",
    "      if ', 0' in cited:\n",
    "        continue\n",
    "\n",
    "      g.add_node(citer_ref, label = citer, title = citer_title)\n",
    "      g.add_node(cited_ref, label = cited, color = rel_color, title = cited_title)\n",
    "      if len(edge_label) == 0:\n",
    "        edge_label = 'No Named Entity found!'\n",
    "      g.add_edge(citer_ref, cited_ref, color = rel_color, width = (edge_width*edge_width), title = edge_label)\n",
    "\n",
    "  g.show(html_name)\n",
    "  display(HTML(html_name))\n",
    "# End of KG generation\n",
    "\n",
    "# For each document get the id of the reference papers to be used as entities\n",
    "def prepare_entities(documents):\n",
    "  for doc in documents:\n",
    "    # Read the JSON data from the file\n",
    "    with open(doc, 'r') as json_file:\n",
    "      data = json.load(json_file)\n",
    "\n",
    "    id = data[\"id\"]\n",
    "    sentences = data[\"sentences\"]\n",
    "\n",
    "    graph = id_sentiment_id(id, sentences)\n",
    "    graphs.append(graph)\n",
    "\n",
    "# Function that gets the documents availble in a folder\n",
    "def get_docs(documents_folder):\n",
    "    files = []\n",
    "    for file_name in os.listdir(documents_folder):\n",
    "        file_path = os.path.join(documents_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            files.append(file_path)\n",
    "    return files\n",
    "\n",
    "# load if only KG is needed to be generated\n",
    "def load_paper_data(file_path):\n",
    "  with open(file_path, 'r') as json_file:\n",
    "    paper_data = json.load(json_file)\n",
    "\n",
    "# Check if a given text contains a citation\n",
    "def has_citation(text):\n",
    "    pattern = re.compile(regex)\n",
    "    match = pattern.search(text)\n",
    "    return match is not None\n",
    "\n",
    "# Change the result of the Sent. Analysis so as to have the score go from ]-1;1[\n",
    "def format_sentiment_result(sen):\n",
    "    data = classifier(sen.text)[0]\n",
    "    sentiment = data[\"label\"]\n",
    "    value = data[\"score\"]\n",
    "\n",
    "    if sentiment == 'negative':\n",
    "      value = -value\n",
    "    elif sentiment == 'neutral':\n",
    "      value = 0\n",
    "\n",
    "    return value\n",
    "\n",
    "# Provided 2 sentences, returns \"contradiction\", \"entailment\" or \"neutral\", based on the context\n",
    "def get_context(sen, inferring_sen):\n",
    "  sequence_to_classify = sen + \" \" + inferring_sen\n",
    "\n",
    "  result = inference(sequence_to_classify, inference_labels)\n",
    "  labels = result['labels']\n",
    "\n",
    "  return labels[0]\n",
    "\n",
    "# If the provided sentence is before the citation sentence and these are in contradiction one to each other\n",
    "# then the provided statement's sentiment analysis result should be the opposite in context\n",
    "# W\n",
    "def get_contextual_result(sentence, context, is_prev_sentence):\n",
    "  result_modifier = 1\n",
    "  sentiment_value = format_sentiment_result(sentence)\n",
    "  if context == 'contradiction':\n",
    "    if is_prev_sentence:\n",
    "      result_modifier = -1\n",
    "\n",
    "  return result_modifier * sentiment_value\n",
    "\n",
    "\n",
    "def prepare_citation_sections(paper_id, sens):\n",
    "  prev_sen = None\n",
    "  prev_context_value = next_context_value = 0\n",
    "  citation_json = []\n",
    "\n",
    "  for sen, next_sen in zip(sens, sens[1:] + [None]):\n",
    "    if has_citation(sen.text):\n",
    "      sen_value = format_sentiment_result(sen)\n",
    "      cited_papers = get_cited_papers(paper_id, sen.text)\n",
    "      citation_section = {}\n",
    "\n",
    "      if prev_sen and not has_citation(prev_sen.text):\n",
    "        context = get_context(prev_sen.text, sen.text)\n",
    "        prev_context_value = 0\n",
    "        if not context == 'neutral':\n",
    "          prev_context_value = get_contextual_result(prev_sen, context, True)\n",
    "          citation_section[-1] = [prev_sen.text, prev_context_value, context]\n",
    "\n",
    "      citation_section[0] = [sen.text, sen_value, cited_papers]\n",
    "\n",
    "      if next_sen and not has_citation(next_sen.text):\n",
    "        context = get_context(sen.text, next_sen.text)\n",
    "        next_context_value = 0\n",
    "        if not context == 'neutral':\n",
    "          next_context_value = get_contextual_result(next_sen, context, False)\n",
    "          citation_section[1] = [next_sen.text, next_context_value, context]\n",
    "\n",
    "      citation_section[\"final_score\"] = sen_value + prev_context_value + next_context_value\n",
    "      citation_json.append(citation_section)\n",
    "\n",
    "  prev_sen = sen\n",
    "\n",
    "  return citation_json\n",
    "\n",
    "\n",
    "def text_to_json(paper_id, text, nlp):\n",
    "    doc = nlp(text)\n",
    "    sens = list(doc.sents)\n",
    "\n",
    "    citation_json = prepare_citation_sections(paper_id, sens)\n",
    "\n",
    "    return citation_json\n",
    "\n",
    "\n",
    "def extract_text_from_file(text_file):\n",
    "    print(f\"Reading {text_file}\")\n",
    "    title = ''\n",
    "\n",
    "    with open(text_file, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    if 'Title: ' in text:\n",
    "      title = text.split('\\n')[0].replace('Title: ', '')\n",
    "      text = text.split(\"\\n\",1)[1].replace('\\n', ' ')\n",
    "\n",
    "    return text, title\n",
    "\n",
    "\n",
    "def save_citation_sections(text_file, json_dir, nlp):\n",
    "    text, paper_id = extract_citing_data(text_file)\n",
    "\n",
    "    citation_json = text_to_json(paper_id, text, nlp)\n",
    "\n",
    "    json_data = {\n",
    "        \"id\": paper_id,\n",
    "        \"sentences\": citation_json\n",
    "    }\n",
    "\n",
    "    file_path = os.path.join(json_dir, paper_id+\".json\")\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(json_data, file, indent=4)\n",
    "\n",
    "    print(f\"  Citation sections written to {file_path}\")\n",
    "\n",
    "\n",
    "def get_citing_author_year(citing_paper_id):\n",
    "  publication_year = re.search(r\"\\d{4}$\", citing_paper_id).group()\n",
    "\n",
    "  author =  citing_paper_id.replace(publication_year, '')\n",
    "\n",
    "  author = re.sub(\"EtAl$\", '', author, flags=re.IGNORECASE).replace(r\".*And.*\", \" and \")\n",
    "\n",
    "  return author, publication_year\n",
    "\n",
    "\n",
    "def extract_citing_data(text_file):\n",
    "  text, title = extract_text_from_file(text_file)\n",
    "  paper_id = os.path.basename(text_file).replace(\".txt\", \"\")\n",
    "  author, publication_year = get_citing_author_year(paper_id)\n",
    "\n",
    "  paper_data[paper_id] = {\n",
    "      \"Author\": author,\n",
    "      \"Title\": title,\n",
    "      \"Year\": publication_year\n",
    "  }\n",
    "\n",
    "  return text, paper_id\n",
    "\n",
    "\n",
    "only_kg_generation = True\n",
    "if not only_kg_generation:\n",
    "  docs = get_docs(input_folder)\n",
    "  sorted_docs = sorted(docs)\n",
    "  cnt = 1\n",
    "  if os.path.exists(output_folder):\n",
    "    while os.path.exists(f\"{output_folder}_({cnt})\"):\n",
    "      cnt += 1\n",
    "    output_folder = output_folder + f\"_({cnt})\"\n",
    "\n",
    "  os.makedirs(output_folder)\n",
    "\n",
    "  for doc in sorted_docs:\n",
    "    if not 'bibliography' in doc:\n",
    "      print(\"_________________________________________________________________________________________________________________________\")\n",
    "      save_citation_sections(doc, output_folder, nlp)\n",
    "\n",
    "print(\"Citations extracted!\")\n",
    "\n",
    "if only_kg_generation:\n",
    "  output_folder = \"./jsonFiles_(2)\"\n",
    "  load_paper_data(\"./citations.json\")\n",
    "\n",
    "json_docs = get_docs(output_folder)\n",
    "prepare_entities(json_docs)\n",
    "\n",
    "if not only_kg_generation:\n",
    "  with open('citations.json', \"w\") as file:\n",
    "    json.dump(paper_data, file, indent=2)\n",
    "\n",
    "visualize_KG('WIP_KG.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
